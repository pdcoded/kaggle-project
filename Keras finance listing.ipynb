{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import operator\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import math\n",
    "os.chdir('/Volumes/pd stuff/Work/kaggle/rental_listing')\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "train_file = \"train.json\"\n",
    "test_file = \"test.json\"\n",
    "train_df = pd.read_json(train_file)\n",
    "test_df = pd.read_json(test_file)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "\n",
    "features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\"]\n",
    "\n",
    "# transformation of lat and lng #\n",
    "train_df[\"price_t\"] = train_df[\"price\"]/train_df[\"bedrooms\"] \n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_dif\"] = train_df[\"bedrooms\"]-train_df[\"bathrooms\"] \n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "train_df[\"price_t1\"] = train_df[\"price\"]/train_df[\"room_sum\"]\n",
    "train_df[\"fold_t1\"] = train_df[\"bedrooms\"]/train_df[\"room_sum\"]\n",
    "\n",
    "test_df[\"room_dif\"] = test_df[\"bedrooms\"]-test_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "test_df[\"price_t1\"] = test_df[\"price\"]/test_df[\"room_sum\"]\n",
    "test_df[\"fold_t1\"] = test_df[\"bedrooms\"]/test_df[\"room_sum\"]\n",
    "\n",
    " \n",
    "# count of photos #\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "# count of \"features\" #\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "# count of words present in description column #\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "# convert the created column to datetime object so as to extract more features \n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "train_df[\"passed\"] = train_df[\"created\"].max() - train_df[\"created\"]\n",
    "train_df['passed']=train_df[\"passed\"].apply(lambda x:re.search(r'\\d+',str(x)).group(0))\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "test_df[\"passed\"] = train_df[\"created\"].max() - test_df[\"created\"]\n",
    "test_df['passed']=test_df[\"passed\"].apply(lambda x:re.search(r'\\d+',str(x)).group(0))\n",
    "train_df['passed']=train_df['passed'].astype(int)\n",
    "test_df['passed']=test_df['passed'].astype(int)\n",
    "\n",
    "\n",
    "# Let us extract some features like year, month, day, hour from date columns #\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df[\"street_number_men\"] = train_df[\"display_address\"].apply(lambda x: int(bool(re.search(r'\\d',x))))\n",
    "test_df[\"street_number_men\"] = test_df[\"display_address\"].apply(lambda x: int(bool(re.search(r'\\d',x))))\n",
    "features_to_use.extend([\"price_t\",\"num_photos\", \"num_features\", \"num_description_words\", \n",
    "                        \"created_year\", \"created_month\", \"created_day\", \"created_hour\",\n",
    "                        \"listing_id\",'price_t1'])\n",
    "ny_lat = 40.785091\n",
    "ny_lon = -73.964613\n",
    "distance_to_ny_train=[]\n",
    "distance_to_ny_test=[]\n",
    "for i in zip(train_df['latitude'],train_df['longitude']):\n",
    "    distance_to_ny_train.append(math.sqrt(pow((i[0]-ny_lat),2) + pow((i[1]-ny_lon),2)))\n",
    "for i in zip(test_df['latitude'],test_df['longitude']):\n",
    "    distance_to_ny_test.append(math.sqrt(pow((i[0]-ny_lat),2) + pow((i[1]-ny_lon),2)))\n",
    "high_lat= 40.748007\n",
    "high_lon= -73.968285\n",
    "distance_to_high_train=[]\n",
    "distance_to_high_test=[]\n",
    "for i in zip(train_df['latitude'],train_df['longitude']):\n",
    "    distance_to_high_train.append(math.sqrt(pow((i[0]-high_lat),2) + pow((i[1]-high_lon),2)))\n",
    "for i in zip(test_df['latitude'],test_df['longitude']):\n",
    "    distance_to_high_test.append(math.sqrt(pow((i[0]-high_lat),2) + pow((i[1]-high_lon),2)))\n",
    "low_lat= 40.739504\n",
    "low_lon= -73.951667\n",
    "distance_to_low_train=[]\n",
    "distance_to_low_test=[]\n",
    "for i in zip(train_df['latitude'],train_df['longitude']):\n",
    "    distance_to_low_train.append(math.sqrt(pow((i[0]-low_lat),2) + pow((i[1]-low_lon),2)))\n",
    "for i in zip(test_df['latitude'],test_df['longitude']):\n",
    "    distance_to_low_test.append(math.sqrt(pow((i[0]-low_lat),2) + pow((i[1]-low_lon),2)))\n",
    "mid_lat= 40.745567\n",
    "mid_lon= -73.965033\n",
    "distance_to_mid_train=[]\n",
    "distance_to_mid_test=[]\n",
    "for i in zip(train_df['latitude'],train_df['longitude']):\n",
    "    distance_to_mid_train.append(math.sqrt(pow((i[0]-mid_lat),2) + pow((i[1]-mid_lon),2)))\n",
    "for i in zip(test_df['latitude'],test_df['longitude']):\n",
    "    distance_to_mid_test.append(math.sqrt(pow((i[0]-mid_lat),2) + pow((i[1]-mid_lon),2)))\n",
    "mean_description_words_high=91.258140\n",
    "mean_description_words_mid=97.733547\n",
    "mean_description_words_low=87.525201\n",
    "word_distance_high_tr=[]\n",
    "word_distance_high_te=[]\n",
    "word_distance_mid_tr=[]\n",
    "word_distance_mid_te=[]\n",
    "word_distance_low_tr=[]\n",
    "word_distance_low_te=[]\n",
    "for i in train_df['num_description_words']:\n",
    "    word_distance_high_tr.append(i-mean_description_words_high)\n",
    "    word_distance_mid_tr.append(i-mean_description_words_mid)\n",
    "    word_distance_low_tr.append(i-mean_description_words_low)\n",
    "for i in test_df['num_description_words']:\n",
    "    word_distance_high_te.append(i-mean_description_words_high)\n",
    "    word_distance_mid_te.append(i-mean_description_words_mid)\n",
    "    word_distance_low_te.append(i-mean_description_words_low)\n",
    "train_df['distance_to_ny_train']=distance_to_ny_train\n",
    "test_df['distance_to_ny_train']=distance_to_ny_test\n",
    "train_df['distance_to_high']=distance_to_high_train\n",
    "test_df['distance_to_high']=distance_to_high_test\n",
    "train_df['distance_to_low']=distance_to_low_train\n",
    "test_df['distance_to_low']=distance_to_low_test\n",
    "train_df['distance_to_mid']=distance_to_mid_train\n",
    "test_df['distance_to_mid']=distance_to_mid_test\n",
    "train_df['word_distance_high']=word_distance_high_tr\n",
    "train_df['word_distance_mid']=word_distance_mid_tr\n",
    "train_df['word_distance_low']=word_distance_low_tr\n",
    "test_df['word_distance_high']=word_distance_high_te\n",
    "test_df['word_distance_mid']=word_distance_mid_te\n",
    "test_df['word_distance_low']=word_distance_low_te\n",
    "features_to_use.extend(['distance_to_ny_train','distance_to_high','distance_to_low','distance_to_mid','word_distance_high','word_distance_mid','word_distance_low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bathrooms                float64\n",
       "bedrooms                   int64\n",
       "latitude                 float64\n",
       "longitude                float64\n",
       "price                      int64\n",
       "price_t                  float64\n",
       "num_photos                 int64\n",
       "num_features               int64\n",
       "num_description_words      int64\n",
       "created_year               int64\n",
       "created_month              int64\n",
       "created_day                int64\n",
       "created_hour               int64\n",
       "listing_id                 int64\n",
       "price_t1                 float64\n",
       "distance_to_ny_train     float64\n",
       "distance_to_high         float64\n",
       "distance_to_low          float64\n",
       "distance_to_mid          float64\n",
       "word_distance_high       float64\n",
       "word_distance_mid        float64\n",
       "word_distance_low        float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[features_to_use].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10                                                         \n",
      "10000     Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "100004    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "100007                               Hardwood_Floors No_Fee\n",
      "100013                                              Pre-War\n",
      "Name: features, dtype: object\n"
     ]
    }
   ],
   "source": [
    "categorical = [\"display_address\", \"manager_id\", \"building_id\", \"street_address\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            #print(f)\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "print(train_df[\"features\"].head())\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=500)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.index=range(train_df.shape[0])\n",
    "test_df.index=range(test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_sparse = pd.DataFrame(tr_sparse.toarray())\n",
    "te_sparse=pd.DataFrame(te_sparse.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X=pd.concat([train_df[features_to_use],tr_sparse],axis=1,ignore_index=True)\n",
    "test_X=pd.concat([test_df[features_to_use],te_sparse],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74659, 526)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49352, 526)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((49352, 526), (74659, 526))\n"
     ]
    }
   ],
   "source": [
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_y =np_utils.to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = sequence.pad_sequences(train_X.as_matrix(), maxlen=train_X.shape[1])\n",
    "test_X = sequence.pad_sequences(test_X.as_matrix(), maxlen=train_X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = sequence.pad_sequences(train_df[features_to_use].as_matrix(), maxlen=train_df[features_to_use].shape[1])\n",
    "test_X = sequence.pad_sequences(test_df[features_to_use].as_matrix(), maxlen=test_df[features_to_use].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X=train_X.as_matrix()\n",
    "test_X=test_X.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(300, input_dim=train_X.shape[1], init='normal', activation='sigmoid'))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(300, init='normal', activation='sigmoid'))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Dense(3, init='normal', activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.2318     \n",
      "Epoch 2/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1978     \n",
      "Epoch 3/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1940     \n",
      "Epoch 4/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1932     \n",
      "Epoch 5/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1924     \n",
      "Epoch 6/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1922     \n",
      "Epoch 7/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1920     \n",
      "Epoch 8/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1921     \n",
      "Epoch 9/1000\n",
      "49352/49352 [==============================] - 3s - loss: 2.1920     \n",
      "Epoch 10/1000\n",
      "49352/49352 [==============================] - 3s - loss: 2.1921     \n",
      "Epoch 11/1000\n",
      "49352/49352 [==============================] - 3s - loss: 2.1921     \n",
      "Epoch 12/1000\n",
      "49352/49352 [==============================] - 3s - loss: 2.1916     \n",
      "Epoch 13/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1924     \n",
      "Epoch 14/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1921     \n",
      "Epoch 15/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1921     \n",
      "Epoch 16/1000\n",
      "49352/49352 [==============================] - 4s - loss: 2.1911     \n",
      "Epoch 17/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1917     \n",
      "Epoch 18/1000\n",
      "49352/49352 [==============================] - 5s - loss: 2.1920     \n",
      "Epoch 19/1000\n",
      "49352/49352 [==============================] - 7s - loss: 2.1919     \n",
      "Epoch 20/1000\n",
      "49352/49352 [==============================] - 6s - loss: 2.1923     \n",
      "Epoch 21/1000\n",
      "13600/49352 [=======>......................] - ETA: 4s - loss: 2.1728"
     ]
    }
   ],
   "source": [
    "class_weight = {0 :8,1:3,2:1}\n",
    "model=baseline_model()\n",
    "model.fit(train_X,train_y,nb_epoch=1000,batch_size=100,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds=model.predict(test_X)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_main_keras_last_stret_number_my_count_features.csv\", index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.04553891,  0.06387437,  0.89058673],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.34433517,  0.32718781,  0.32847703],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692],\n",
       "       [ 0.30399293,  0.34628016,  0.34972692]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0:100,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['interest_level'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "34284/3839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "34284/11229"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
